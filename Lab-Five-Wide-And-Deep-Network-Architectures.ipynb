{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e3d100f6",
   "metadata": {},
   "source": [
    "# Lab Assignment Five: Wide and Deep Network Architectures"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "77a21953",
   "metadata": {},
   "source": [
    "In this lab, we will select a prediction task to perform on our dataset, evaluate two different deep learning architectures and tune hyper-parameters for each architecture."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f23ab909",
   "metadata": {},
   "source": [
    "## Team Members:\n",
    "1) Mohammed Ahmed Abdelrazek Aboelela.\n",
    "\n",
    "2) Naim Barnett"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e93defd6",
   "metadata": {},
   "source": [
    "## Dataset Selection"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9eccb57c",
   "metadata": {},
   "source": [
    "Data Set : Credit Card Classification - https://www.kaggle.com/datasets/parisrohan/credit-score-classification?select=train.csv"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "38149cbf",
   "metadata": {},
   "source": [
    "### Overview and Business Understanding"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "66ceaae8",
   "metadata": {},
   "source": [
    "It is very important in the work of global financial companies and also banks to have a classifier that helps them to decide whether or not to trust customers when lending them large sum of money, such as a mortgage or a line of credit. To determine said reliability of customers, companies and banks utilizes the credit score, which is dependent on a variety of factors. Within the public domain of Kaggle, there is a large database that holds information on the known factors associated with credit score, and the final given credit score bracket. It contains over 100000 datapoints, and it utilizes both numeric and categorical data. Our goal is to build an intelligent system to segregate the people into credit score brackets to reduce the manual efforts. Thus, the main prediction task here is to classify the credit score of a customer based on their credit-related attributes. This is of direct interest to third parties (such as companies) that want a tool to reduce the efforts to classify their customer's credit scores. Consequently, for the prediction algorithm to be considered useful, it needs to be very efficient when applied to our test data in predicting the credit scores of the test customers. The model (from my own understanding of the difference between online and offline analysis) will be mostly for offline analysis, meaning that the model will be trained and tested using the already provided datapoints, and then the prediction data will be collected and fed to the algorithm that will predict the respective credit score bracket."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1752365f",
   "metadata": {},
   "source": [
    "## Preparation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c93a3e4e",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"Importing all the needed packages\"\"\"\n",
    "import numpy as np \n",
    "import pandas as pd\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "from copy import deepcopy\n",
    "import seaborn as sns\n",
    "import warnings\n",
    "warnings.simplefilter('ignore', DeprecationWarning)\n",
    "import re\n",
    "import missingno as mn         #make sure to have the package installed \"pip install missingno\"\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "import pprint as pp\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from sklearn.metrics import make_scorer, accuracy_score, precision_score\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn import metrics as mt\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "import os\n",
    "os.environ['AUTOGRAPH_VERBOSITY'] = '0'\n",
    "\n",
    "print(tf.__version__)\n",
    "print(keras.__version__)\n",
    "\n",
    "from tensorflow.keras.layers import Dense, Activation, Input\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.utils import plot_model\n",
    "from tensorflow.keras.layers import Embedding\n",
    "from tensorflow.keras.layers import concatenate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "70caf61f",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "\"\"\"Loading the dataset\"\"\"\n",
    "df_train_orig = pd.read_csv('train.csv', low_memory=False)\n",
    "df_train_orig.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "81c553ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = deepcopy(df_train_orig)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "19067671",
   "metadata": {},
   "source": [
    "From the data below, we can see the overall statistics from the raw data. Ideally, once we are done cleaning, such values as the average will become more accurate."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e0bd285f",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Showing the initial form of the data and their related features and averages\n",
    "data.info()\n",
    "data.describe().T"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a62c8d22",
   "metadata": {},
   "source": [
    "Below is a table that includes a description of each attribute in our dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "98fb8497",
   "metadata": {},
   "source": [
    "| ID | Customer_ID | Month | Name | Age | SSN | Occupation | Annual_Income | Monthly_Inhand_Salary | Num_Bank_Accounts | Num_Credit_Card | Interest_Rate| Num_of_Loan | Type_of_Loan | Delay_from_due_date | Num_of_Delayed_Payments | Changed_Credit_Limit | Num_Credit_Inquiries | Credit_Mix | Outstanding_Debt | Credit_Utilization_Ratio | Credit_History_Age | Payment_of_Min_Amount | Total_EMI_per_month | Amount_invested_monthy | Payment_Behaviour | Monthly_Balance | Credit_Score |\n",
    "| --- | --- | --- | --- | --- | --- | --- | --- | --- | --- | --- | --- | --- | --- | --- | --- | --- | --- | --- | --- | --- | --- | --- | --- | --- | --- | --- | --- |\n",
    "| a unique identification of an entry | a unique identification of a person | the month of the year | the name of a person | the age of the person | the social security number of the person | the occupation of the person | the annual income of the person | the monthly base salary of a person | the number of bank accounts a person holds | the number of other credit cards held by a person | the interest rate on the credit card | the number of loans taken from the bank | the types of loan taken by a person | the average number of days delayed from the payment date | the average number of payments delayed by a person | the percentage change in credit card limit | the number of credit card inquiries | the classification of the mix of credits (the types of different credit accounts) | the remaining debt to be paid (in USD) | the utilization ratio of credit card (the sum of all your balances, divided by the sum of your cards' credit limits) | the age of credit history of the person | whether only the minimum amount was paid by the person | the monthly EMI \"Equated monthly installment\" payments (in USD) | the monthly amount invested by the customer (in USD) | the payment behavior of the customer (in USD) | the monthly balance amount of the customer (in USD) | the bracket of credit score (Poor, Standard, Good)|"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "022db2f3",
   "metadata": {},
   "source": [
    "Due to the relatively big number of attributes (27 + Credit Score), we will be more inclined to get rid of some of them if: \n",
    "1) They are not relevant to our analysis.\n",
    "\n",
    "2) They have a big number of missing data (which make it hard to do imputation)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3dd4c0b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Looking at the overall shape of the data\n",
    "mn.matrix(data)\n",
    "plt.title(\"Visualization of the overall data shape\", fontsize=30)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad192ec4",
   "metadata": {},
   "source": [
    "To ensure our data maintains high quality, so our results can be as accurate as possible, we need to clean our data.\n",
    "\n",
    "After an attempt to impute the null or unusable values, we found that it ended up skewing the values. Additionally the sheer amount of values caused overfitting in some of our calculations. Resultantly, we chose to simply remove the unusable data .\n",
    "\n",
    "First, we are going to remove columns that are not useful to our analysis. We can see that information such as Customer_ID, Month, Name, and SSN are general information that is extremely unlikely to have any affect on the trends we are analyzing. As a result, we can remove the columns to narrow our dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "877a4cd3",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Print column names\n",
    "data.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf0bcf48",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''Remove Customer_ID, Name, SSN, Month, and Type_of_Loan. We remove the \"Type_of_Loan\" because it has broadly many unique values\n",
    "and combinations that will be very hard to trace and will more likely make it harder for the network to find a pattern in training. We remove\n",
    "Occupation because becomes a redundant feature in conjunction with \"Income\". \"Income\" is more applicable to our model.\n",
    "'''\n",
    "\n",
    "data.drop(['ID','Customer_ID', 'Name', 'SSN', 'Type_of_Loan', 'Month'], axis=1, inplace=True)\n",
    "data.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b51ce145",
   "metadata": {},
   "source": [
    "Secondly, we are going to fill null values in the columns missing data. We also want to remove any illegal values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f2d6298b",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Replace Invalid Values\n",
    "data =data.replace(r'[^\\w\\s.]|_|-', '', regex=True) \n",
    "#Replace all blank strings will null to be dropped\n",
    "data.replace('', np.nan, inplace=True)\n",
    "#Remove all rows with null values\n",
    "data.dropna(inplace=True);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "93e7414f",
   "metadata": {},
   "outputs": [],
   "source": [
    "data.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a6271885",
   "metadata": {},
   "source": [
    "Thirdly, we want to clear out all duplicate data so our frequency analysis remains accurate."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "92121984",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Find duplicate instances\n",
    "duplicates = data[data.duplicated()]\n",
    "\n",
    "#Remove all duplicates\n",
    "data = data.drop_duplicates()\n",
    "\n",
    "data.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ba44afa5",
   "metadata": {},
   "source": [
    "Fourthly, we would like to remove outliers from out dataset, so that our data analysis isn't skewed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "87651859",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Determine outliers\n",
    "Q1 = data.quantile(0.25)\n",
    "Q3 = data.quantile(0.75)\n",
    "IQR = Q3 - Q1\n",
    "#Remove outliers\n",
    "data = data[~((data < (Q1 - 1.5*IQR)) | (data > (Q3 + 1.5*IQR))).any(axis = 1)]\n",
    "data = data[(data['Num_Bank_Accounts'] >= 0)]\n",
    "\n",
    "data[['Age']] = data[['Age']].apply(pd.to_numeric)\n",
    "data = data.loc[(data[\"Age\"] > 0) & (data[\"Age\"] <= 112)] #112 is the recorded oldest age ever!\n",
    "\n",
    "data[['Changed_Credit_Limit']] = data[['Changed_Credit_Limit']].apply(pd.to_numeric)\n",
    "data = data.loc[(data[\"Changed_Credit_Limit\"] > 0) & (data[\"Changed_Credit_Limit\"] <= 100)] #Since it's a percentage,\n",
    "#we do the selection of the instances with numbers between 0 and 100%.\n",
    "\n",
    "data[['Monthly_Balance']] = data[['Monthly_Balance']].astype('float64')\n",
    "data = data.loc[(data[\"Monthly_Balance\"] < 10000)] #Keeping the reasonable monthly balance (value < 10000), \n",
    "#and the converting to numeric values\n",
    "\n",
    "data = data.loc[(data[\"Payment_Behaviour\"] != '98')] #noticed this unreasonable category thus removing it\n",
    "\n",
    "data = data.loc[(data[\"Payment_of_Min_Amount\"] != 'NM')] #keeping only known info about payment of min amount\n",
    "\n",
    "data.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dfcc9aeb",
   "metadata": {},
   "source": [
    "Fifthly, we convert all the values that unreasonably categorical into numeric for convinience."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae2fce19",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"Converting the unreasonable categorical features to numeric\"\"\"\n",
    "data[['Annual_Income']] = data[['Annual_Income']].apply(pd.to_numeric)\n",
    "data[['Num_of_Loan']] = data[['Num_of_Loan']].apply(pd.to_numeric)\n",
    "data[['Num_of_Delayed_Payment']] = data[['Num_of_Delayed_Payment']].apply(pd.to_numeric)\n",
    "data[['Outstanding_Debt']] = data[['Outstanding_Debt']].apply(pd.to_numeric)\n",
    "data[['Outstanding_Debt']] = data[['Outstanding_Debt']].apply(pd.to_numeric)\n",
    "data['Credit_History_Age'] = data['Credit_History_Age'].str[:2] #Keeping the year part only\n",
    "data[['Credit_History_Age']] = data[['Credit_History_Age']].apply(pd.to_numeric)\n",
    "data[['Amount_invested_monthly']] = data[['Amount_invested_monthly']].apply(pd.to_numeric)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cdcf4e4e",
   "metadata": {},
   "outputs": [],
   "source": [
    "data.reset_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "60304041",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "data.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "247f2774",
   "metadata": {},
   "source": [
    "Our clean data then becomes: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a5326f5f",
   "metadata": {},
   "outputs": [],
   "source": [
    "mn.matrix(data)\n",
    "plt.title(\"Post-Cleaning\", fontsize=30)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "83af3d26",
   "metadata": {},
   "outputs": [],
   "source": [
    "data.describe().T"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3fa79d66",
   "metadata": {},
   "source": [
    "Checking categorical variables, grouping them and also grouping the numerical ones together."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1cedd4e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "categorical_vars = []\n",
    "numerical_vars = []\n",
    "for column in data.columns:\n",
    "    if data[column].dtype == 'object' and column != \"Credit_Score\":\n",
    "        categorical_vars.append(column)\n",
    "    else:\n",
    "        if column != \"Credit_Score\": numerical_vars.append(column)\n",
    "        \n",
    "print(\"The categorical variables in our cleaned dataset are:\", categorical_vars)\n",
    "print(\"The numerical variables in our cleaned dataset are:\", numerical_vars)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "41976803",
   "metadata": {},
   "source": [
    "Performing the usual standard scaling on numerical features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "415e9d7f",
   "metadata": {},
   "outputs": [],
   "source": [
    "ss = StandardScaler()\n",
    "data[numerical_vars] = ss.fit_transform(data[numerical_vars].values)\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "86d42e61",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#COME BACK LATER IF WE HAVE TIME\n",
    "\"\"\"Creating a heatmap to see which numerical variables are mostly correlated to help in dimensionality reduction (if we will do it)\"\"\"\n",
    "#sns.heatmap(data.corr(),annot=True)\n",
    "#plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eab209c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "data[\"Credit_Score\"].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "32ac5195",
   "metadata": {},
   "source": [
    "Encoding the categorical features as integers using the label encoder from scikit learn."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "86e00f2a",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"CHECK ONE-HOT ENCODING INSTEAD OF NORMAL ENCODER LATER HERE IF YOU HAVE TIME\"\"\"\n",
    "# define objects that can encode each variable as integer    \n",
    "encoders = dict() # save each encoder in dictionary\n",
    "# train all encoders (special case the target 'income')\n",
    "for col in categorical_vars+['Credit_Score']:\n",
    "    data[col] = data[col].str.strip()\n",
    "    \n",
    "    if col==\"Credit_Score\":\n",
    "        # special case the target, just replace the column\n",
    "        tmp = LabelEncoder()\n",
    "        data[col] = tmp.fit_transform(data[col])\n",
    "    else:\n",
    "        # integer encode strings that are features\n",
    "        encoders[col] = LabelEncoder() # save the encoder\n",
    "        data[col+'_int'] = encoders[col].fit_transform(data[col])\n",
    "        \n",
    "\n",
    "#Container for the names of our categorical encoded features\n",
    "categorical_vars_ints = [x+'_int' for x in categorical_vars]\n",
    "\n",
    "#Collecting together the features we will be interested in using later\n",
    "feature_columns = categorical_vars_ints+numerical_vars\n",
    "\n",
    "print(f\"We will use the following {len(feature_columns)} features:\")\n",
    "pp.pprint(feature_columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7dd1a754",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3fb2631a",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "data.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9774bfb9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# we have the following lists now of data that we can use with our dataframes:\n",
    "print(\"Numeric Headers:\")\n",
    "pp.pprint(numerical_vars) # normalized numeric data\n",
    "print(\"\\nCategorical String Headers:\")\n",
    "pp.pprint(categorical_vars) # string data\n",
    "print(\"\\nCategorical Headers, Encoded as Integer:\")\n",
    "pp.pprint(categorical_vars_ints) # string data encoded as an integer"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "594f1124",
   "metadata": {},
   "source": [
    "Thus, the final pre-processed clean dataset we have consists of 31,212 instances. It has 17 numerical features that are ['Age', 'Annual_Income', 'Monthly_Inhand_Salary', 'Num_Bank_Accounts', 'Num_Credit_Card', 'Interest_Rate', 'Num_of_Loan', 'Delay_from_due_date', 'Num_of_Delayed_Payment', 'Changed_Credit_Limit', 'Num_Credit_Inquiries', 'Outstanding_Debt', 'Credit_Utilization_Ratio', 'Credit_History_Age', 'Total_EMI_per_month', 'Amount_invested_monthly', 'Monthly_Balance'] and 4 categorical ones ['Occupation', 'Credit_Mix', 'Payment_of_Min_Amount', 'Payment_Behaviour']. The task is to perform a classification and predict the target variable that is ['Credit_Score'], which is encoded too using the same label encoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "75edc0bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# sandbox for looking at different categorical variables\n",
    "for col in categorical_vars:\n",
    "    vals = data[col].unique()\n",
    "    print(col,'has', len(vals), 'unique values:')\n",
    "    print(vals)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b2adfd6f",
   "metadata": {},
   "source": [
    "We will now look to combine related features into cross-product features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3cb26602",
   "metadata": {},
   "outputs": [],
   "source": [
    "# choose these as a class, what makes sense\n",
    "cross_columns = ['Credit_Mix', 'Payment_of_Min_Amount', 'Payment_Behaviour']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "050f71ba",
   "metadata": {},
   "source": [
    "For our crossed columns, we chose to cross 'Credit_Mix', 'Payment_of_Min_Amount', and 'Payment_Behaviour' because we think that we can gain new knowledge by making combinations of these sort-of realated payment features. However, we see really no relevance in including the 'occupation' in any of the crossings as it does not have any reasonable relationship to customer's credit behavior or add new knowledge to what we have currently. We can also cross any two of them together later. We're going to investigate this in the modeling part"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3f351f20",
   "metadata": {},
   "source": [
    "Before everything, we are going to do a small modification to our target variable. Our problem is mainly labeled as a multi-class classification, but when you actually think about, as a company, you are only concerned about knowing whether a person has a \"poor\" credit score or not to be able to determine whether to give them the loan or not. You don't really care whether their credit score is \"Standard\" or \"Good\" which are both deemed to be good enough to be given loans. So, this compels us to change the nature of our prediction class to a \"binary\" one, where we will be mainly interested in determining whether the targer variable will be \"poor: 0\" or \"not poor:1\". We implement this below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e3a84d83",
   "metadata": {},
   "outputs": [],
   "source": [
    "data[\"Credit_Score\"].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27db153b",
   "metadata": {},
   "outputs": [],
   "source": [
    "data[\"Credit_Score\"].replace([1,0,2],[0,1,1], inplace=True)\n",
    "data[\"Credit_Score\"].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "64041d33",
   "metadata": {},
   "source": [
    "Given our current binary classification problem, the appropriate metric we decided to use is the \"Precision\" metric. This reasoning behind this is that we are only interested that we maximize the true positive out of the total predicted positives (with positive = the person being predicted to be not poor, thus we will be giving money as a company). We don't really care about whether the predicted negative is true or false cause in any case, we will not be losing money if we decide against giving the loans anyway. Thus, precision which is the ratio between the true predicted positives to the total predicted positives is the right metric we will want to maximize."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "070ce114",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"Separating the features and the target variable in the dataframe\"\"\"\n",
    "X, y = data[feature_columns+categorical_vars], data['Credit_Score']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23a8ca6d",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"Separating into testing and training samples\"\"\"\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=0, stratify=y)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f8786256",
   "metadata": {},
   "source": [
    "For our model, we will choose to use Stratified 10-fold cross validation to ensure we uphold an even distribution across every fold. This will allow us to train our model numerous times with a variation of training and test sets. We think Stratified 10-fold is what we want because we don't want to do shuffle splits before each iteration as Shuffle splits/StratifiedShuffleSplit might do which might result in repeated training instances in some of the iterations, as we do not have a relatively huge sample to not care about this happening."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d82d8c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "skf = StratifiedKFold(n_splits=10)\n",
    "skf.get_n_splits(X_train, y_train)\n",
    "print(skf)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "68822190",
   "metadata": {},
   "source": [
    "## Modeling (5 points total)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e50f28b2",
   "metadata": {},
   "source": [
    "[2 points] Create at least three combined wide and deep networks to classify your data using Keras. Visualize the performance of the network on the training data and validation data in the same plot versus the training iterations. Note: use the \"history\" return parameter that is part of Keras \"fit\" function to easily access this data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d2523abd",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "\"\"\"Investigating the crossed columns\"\"\"\n",
    "cols_list = cross_columns\n",
    "\n",
    "# 1. create crossed labels by string join operation\n",
    "X_crossed_train = X_train[cols_list].apply(lambda x: '_'.join(x), axis=1)\n",
    "X_crossed_test = X_test[cols_list].apply(lambda x: '_'.join(x), axis=1)\n",
    "\n",
    "# combine together for training\n",
    "all_vals = np.hstack((X_crossed_train.to_numpy(),  X_crossed_test.to_numpy()))\n",
    "print(np.unique(all_vals))\n",
    "    \n",
    "# 2. encode as integers, stacking all possibilities\n",
    "enc = LabelEncoder()\n",
    "enc.fit(all_vals)\n",
    "\n",
    "encoded_vals_train = enc.transform(X_crossed_train)\n",
    "encoded_vals_test  = enc.transform(X_crossed_test)\n",
    "\n",
    "print(np.min(encoded_vals_train), np.max(encoded_vals_train))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2cf866ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "# now let's create some different crossed values\n",
    "cross_columns = [['Credit_Mix', 'Payment_of_Min_Amount', 'Payment_Behaviour'],\n",
    "                 ['Credit_Mix','Payment_of_Min_Amount'],\n",
    "                 ['Credit_Mix','Payment_Behaviour'],\n",
    "                 ['Payment_of_Min_Amount','Payment_Behaviour']\n",
    "                ]\n",
    "\n",
    "\n",
    "# cross each set of columns in the list above\n",
    "cross_col_df_names = []\n",
    "for cols_list in cross_columns:\n",
    "    # encode as ints for the embedding\n",
    "    enc = LabelEncoder()\n",
    "    \n",
    "    # 1. create crossed labels by join operation\n",
    "    X_crossed_train = X_train[cols_list].apply(lambda x: '_'.join(x), axis=1)\n",
    "    X_crossed_test = X_test[cols_list].apply(lambda x: '_'.join(x), axis=1)\n",
    "    \n",
    "    # get a nice name for this new crossed column\n",
    "    cross_col_name = '_'.join(cols_list)\n",
    "    \n",
    "    # 2. encode as integers, stacking all possibilities\n",
    "    enc.fit(np.hstack((X_crossed_train.to_numpy(),  X_crossed_test.to_numpy())))\n",
    "    \n",
    "    # 3. Save into dataframe with new name\n",
    "    X_train[cross_col_name] = enc.transform(X_crossed_train)\n",
    "    X_test[cross_col_name] = enc.transform(X_crossed_test)\n",
    "    \n",
    "    # Save the encoder used here for later:\n",
    "    encoders[cross_col_name] = enc\n",
    "    \n",
    "    # keep track of the new names of the crossed columns\n",
    "    cross_col_df_names.append(cross_col_name) \n",
    "    \n",
    "cross_col_df_names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17f749b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train a model only using crossed values\n",
    "# get crossed columns\n",
    "X_train_crossed = X_train[cross_col_df_names].to_numpy()\n",
    "X_test_crossed = X_test[cross_col_df_names].to_numpy()\n",
    "\n",
    "crossed_outputs = [] # this is where we will keep track of output of each branch\n",
    "\n",
    "input_crossed = Input(shape=(X_train_crossed.shape[1],), dtype='int64', name='categorical')\n",
    "for idx,col in enumerate(cross_col_df_names):\n",
    "    \n",
    "    # track what the maximum integer value will be for this variable\n",
    "    # which is the same as the number of categories\n",
    "    N = max(X_train[col].max(),df_test[col].max())+1\n",
    "    N = len(encoders[col].classes_)\n",
    "    N_reduced = int(np.sqrt(N))\n",
    "    \n",
    "    # this line of code does this: input_branch[:,idx]\n",
    "    x = tf.gather(input_crossed, idx, axis=1)\n",
    "    \n",
    "    # now use an embedding to deal with integers as if they were one hot encoded\n",
    "    x = Embedding(input_dim=N, \n",
    "                  output_dim=N_reduced, \n",
    "                  input_length=1, name=col+'_embed')(x)\n",
    "    \n",
    "    # save these outputs to concatenate later\n",
    "    crossed_outputs.append(x)\n",
    "    \n",
    "\n",
    "# now concatenate the outputs and add a fully connected layer\n",
    "wide_branch = concatenate(crossed_outputs, name='concat_1')\n",
    "wide_branch = Dense(units=1,activation='sigmoid', name='combined')(wide_branch)\n",
    "\n",
    "model = Model(inputs=input_crossed, outputs=wide_branch)\n",
    "\n",
    "model.compile(optimizer='sgd',\n",
    "              loss='mean_squared_error',\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "model.fit(X_train_crossed,\n",
    "        y_train, epochs=10, batch_size=32, verbose=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "efdefddf",
   "metadata": {},
   "source": [
    "[2 points] Investigate generalization performance by altering the number of layers in the deep branch of the network. Try at least two different number of layers. Use the method of cross validation and evaluation metric that you argued for at the beginning of the lab to select the number of layers that performs superiorly. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "63f71aa5",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "dd4e9f87",
   "metadata": {},
   "source": [
    "[1 points] Compare the performance of your best wide and deep network to a standard multi-layer perceptron (MLP). Alternatively, you can compare to a network without the wide branch (i.e., just the deep network). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "236abb58",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "045e09d8",
   "metadata": {},
   "source": [
    "## Exceptional Work (1 points total)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "79839573",
   "metadata": {},
   "source": [
    "5000 students: You have free rein to provide additional analyses."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7fbd9876",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "f717a775",
   "metadata": {},
   "source": [
    "One idea (required for 7000 level students): For classification tasks, compare using the receiver operating characteristic and area under the curve. For regression tasks, use Bland-Altman plots and residual variance calculations.  Use proper statistical methods to compare the performance of different models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c148f6e",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  },
  "vscode": {
   "interpreter": {
    "hash": "5e760e3d3e77aa6826ef4f79166ae45bbfcee688d0233fb93e5618e0e742b754"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
